
x-common-env: &common-env
  TZ: America/Chicago
  PUID: 1000
  PGID: 1000

services:
  nginx:
    image: nginx:alpine
    container_name: invoiceninja_nginx
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 256M
    ports:
      - "9000:80" # InvoiceNinja UI exposed here
    volumes:
      - ninja_public_data:/var/www/app/public # For InvoiceNinja public assets
      - ./nginx/nginx.conf:/etc/nginx/conf.d/default.conf:ro
    networks:
      - internal_network
    depends_on:
      invoiceninja:
        condition: service_started
    environment:
      - TZ=America/Chicago

  invoiceninja:
    image: invoiceninja/invoiceninja:latest
    container_name: invoiceninja
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 768M
    environment:
      - TZ=America/Chicago
      - APP_URL=http://localhost:9000
      - APP_KEY=base64:feaoxTH+JPT6jQ3YYX14JLiBUxe3jFpAm701XW2CZCE=
      - DB_CONNECTION=sqlite
      - REQUIRE_HTTPS=false
      - APP_DEBUG=${INVOICENINJA_APP_DEBUG:-false}
      - APP_ENV=${INVOICENINJA_APP_ENV:-production}
      - LOG_LEVEL=error
      - SESSION_DRIVER=file
      - CACHE_DRIVER=file
      - QUEUE_CONNECTION=database
      - MAIL_MAILER=log
      - TRUSTED_PROXIES=*
    volumes:
      - ninja_public_data:/var/www/app/public
      - ninja_storage_data:/var/www/app/storage
    networks:
      - internal_network

  backend_assistant:
    build:
      context: ./backend_assistant
      dockerfile: Dockerfile
    container_name: backend_assistant
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: '0.75'
          memory: 512M
    ports:
      - "8001:8001"
    env_file:
      - ./backend_assistant/.env.assistant
    volumes:
      - assistant_data:/app/data
      - assistant_config:/app/config
      # If you created specific files like client_billing_rates.json in ./backend_assistant/config
      # you can mount them directly:
      - ./backend_assistant/config/client_billing_rates.json:/app/config/client_billing_rates.json:ro
      # - ./backend_assistant/config/google-service-account.json:/app/config/google-service-account.json:ro # If you add it
    networks:
      - internal_network
    depends_on:
      invoiceninja:
        condition: service_started
    healthcheck:
        test: ["CMD", "curl", "-f", "http://localhost:8001/health"]
        interval: 30s
        timeout: 10s
        retries: 3
        start_period: 30s

  open-webui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: openwebui
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: '1.5'
          memory: 2G
    ports:
      - "3000:8080"
    environment:
      - TZ=America/Chicago
      # Update this to where your LLM (e.g., Ollama, or other OpenAI compatible API) is running
      # If llm-runner service is defined and on same network: http://llm-runner:<port>/v1
      # If Ollama running on host and accessible: http://host.docker.internal:11434/v1 (for Docker Desktop)
      - OPENAI_API_BASE_URL=http://host.docker.internal:11434/v1
      - OPENAI_API_KEY=ollama # For Ollama, the key can be 'ollama' or any string
      - WEBUI_NAME=InvoiceNinja AI Assistant
      - WEBUI_URL=http://localhost:3000
      - BACKEND_ASSISTANT_URL=http://backend_assistant:8001 # For OpenWebUI to call assistant
    volumes:
      - openwebui_data:/app/backend/data
      # You might need to map specific OpenWebUI config files if you customize them
      # - ./openwebui_config.json:/app/backend/config.json # Example from your original compose
    networks:
      - internal_network
    depends_on:
      - backend_assistant # OpenWebUI starts after the assistant
      # - llm-runner # Uncomment if you define and use a local llm-runner service
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

  # llm-runner: # Example for a local Ollama service, if not running Ollama elsewhere
  #   image: ollama/ollama:latest
  #   container_name: llm_runner
  #   restart: unless-stopped
  #   ports:
  #     - "11434:11434"
  #   volumes:
  #     - ollama_data:/root/.ollama
  #   networks:
  #     - internal_network
  #   # Add any other Ollama specific configurations, like GPU deployment if needed
  #   # deploy:
  #   #   resources:
  #   #     reservations:
  #   #       devices:
  #   #         - driver: nvidia
  #   #           count: 1
  #   #           capabilities: [gpu]

networks:
  internal_network:
    driver: bridge

volumes:
  ninja_public_data:
  ninja_storage_data:
  openwebui_data:
  assistant_data:
  assistant_config:
  # ollama_data: # Uncomment if you use the example llm-runner service for Ollama