
services:
  nginx:
    image: nginx:alpine
    container_name: invoiceninja_nginx
    restart: unless-stopped
    ports:
      - "9000:80"
    volumes:
      - ninja_data:/var/www/app/public
      - ./nginx.conf:/etc/nginx/conf.d/default.conf
    networks:
      - internal_network
    depends_on:
      - invoiceninja

  invoiceninja:
    image: invoiceninja/invoiceninja:latest
    container_name: invoiceninja
    restart: unless-stopped
    environment:
      - APP_URL=http://localhost:9000
      - APP_KEY=base64:feaoxTH+JPT6jQ3YYX14JLiBUxe3jFpAm701XW2CZCE=
      - DB_CONNECTION=sqlite
      - DB_DATABASE=/var/www/app/database/database.sqlite
    volumes:
      - ninja_data:/var/www/app/public
      - ./database:/var/www/app/database
      - ./storage:/var/www/app/storage
      - DB_CONNECTION=mysql
      - DB_HOST=mysql
      - DB_PORT=3306
      - DB_DATABASE=invoiceninja
      - DB_USERNAME=invoiceninja
      - DB_PASSWORD=invoiceninja123
      - REQUIRE_HTTPS=false
      - APP_DEBUG=true
      - APP_ENV=local
    volumes:
      - ninja_data:/var/www/app/public
      - ninja_db:/var/www/app/storage
    networks:
      - internal_network
    depends_on:
      - mysql

  ai-assistant:
    build: .
    container_name: ai-invoice-assistant
    restart: unless-stopped
    volumes:
      - ./gpt-large:/app/gpt-large  # Local model files
      - ./model_cache:/app/model_cache  # Model cache directory
      - ./offload:/app/offload  # Offloaded model layers
      - ./app:/app/app  # Application code
      - ./model_cache/huggingface:/root/.cache/huggingface  # Hugging Face cache
    environment:
      # Email Configuration
      - EMAIL_SERVER=${EMAIL_SERVER}
      - EMAIL_PORT=${EMAIL_PORT}
      - EMAIL_USERNAME=${EMAIL_USERNAME}
      - EMAIL_PASSWORD=${EMAIL_PASSWORD}
      - EMAIL_FOLDER=${EMAIL_FOLDER}
      
      # InvoiceNinja Configuration
      - INVOICE_NINJA_URL=http://localhost:9000
      - INVOICE_NINJA_TOKEN=${INVOICE_NINJA_TOKEN}
      
      # LLM Backend Configuration
      - LLM_BACKEND=${LLM_BACKEND:-airllm}
      
      # Model Configuration (Transformers)
      - MODEL_PATH=${MODEL_PATH:-./gpt-large}
      - MAX_INPUT_LENGTH=${MAX_INPUT_LENGTH:-1024}
      - MAX_GENERATION_LENGTH=${MAX_GENERATION_LENGTH:-500}
      
      # AirLLM Configuration
      - MODEL_NAME=${MODEL_NAME:-/app/gpt-large}  # Using local model files
      - MODEL_CACHE_DIR=${MODEL_CACHE_DIR:-/app/model_cache}
      - MODEL_OFFLOAD_DIR=${MODEL_OFFLOAD_DIR:-/app/offload}
      - TRUST_REMOTE_CODE=${TRUST_REMOTE_CODE:-true}
      - TORCH_DTYPE=${TORCH_DTYPE:-auto}
      - MAX_SEQ_LENGTH=${MAX_SEQ_LENGTH:-4096}
      - USE_GPU=${USE_GPU:-true}
      - GPU_MEMORY_UTILIZATION=${GPU_MEMORY_UTILIZATION:-0.9}
      - OFFLOAD_LAYERS_RATIO=${OFFLOAD_LAYERS_RATIO:-0.7}
      - OFFLOAD_LAYERS_BUFFER=${OFFLOAD_LAYERS_BUFFER:-4}
      - USE_SAFETENSORS=${USE_SAFETENSORS:-true}
      - USE_FLASH_ATTENTION=${USE_FLASH_ATTENTION:-false}
      
      # Application Settings - Poll every 5 minutes by default
      - POLL_INTERVAL=300
      - TRANSFORMERS_CACHE=/app/model_cache
      - HF_HOME=/app/model_cache
    ports:
      - "8000:8000"
    networks:
      - internal_network
    depends_on:
      - invoiceninja
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 20s

networks:
  internal_network:
    driver: bridge
    
volumes:
  ninja_data:
  ninja_db:
  mysql_data:
#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 hf token
#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
